{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0351cf-df8f-4f90-9289-7d2c7764189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "Recording finished.\n",
      "Batch file embeddings_batch_13.npy not found. Skipping.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "import threading\n",
    "import tkinter as tk\n",
    "from tkinter import font as tkfont\n",
    "from PIL import Image, ImageTk\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.spatial.distance import cosine\n",
    "from keras_vggface.utils import preprocess_input\n",
    "import tensorflow as tf\n",
    "# Load models and embeddings\n",
    "liveness_model = load_model('liveness_model.keras', compile=False)\n",
    "face_model = load_model('face_verification_model.keras', compile=False)\n",
    "face_embeddings = np.load('my_embeddings.npy')\n",
    "face_labels = np.load('my_labels.npy')\n",
    "\n",
    "# Global Variables for GUI elements\n",
    "record_button = None\n",
    "status_label = None\n",
    "video_label = None\n",
    "window = None\n",
    "\n",
    "# Function to preprocess frame for liveness detection\n",
    "def preprocess_frame(frame):\n",
    "    frame_resized = cv2.resize(frame, (224,224))\n",
    "    frame_normalized = frame_resized / 255.0\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)\n",
    "    return frame_expanded\n",
    "\n",
    "# Function to predict liveness\n",
    "def predict_liveness(frame, model):\n",
    "    preprocessed_frame = preprocess_frame(frame)\n",
    "    prediction = model.predict(preprocessed_frame)\n",
    "    return prediction[0][0]\n",
    "\n",
    "# Function to detect faces in the frame\n",
    "def detect_faces(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    return faces\n",
    "\n",
    "# Function to extract and preprocess detected faces\n",
    "def extract_faces(frame, faces):\n",
    "    face_images = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        face_images.append(face)\n",
    "    return face_images\n",
    "\n",
    "# Function to get embedding from the face\n",
    "def preprocess_image(image):\n",
    "    image_resized = cv2.resize(image, (224, 224))\n",
    "    image_array = np.array(image_resized, dtype=np.float32)\n",
    "    image_array = preprocess_input(image_array, version=2)\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "    return image_array\n",
    "\n",
    "def get_embedding(image, model):\n",
    "    image_preprocessed = preprocess_image(image)\n",
    "    embedding = model.predict(image_preprocessed)\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Function to find closest matching face\n",
    "def find_closest_face(query_embedding, face_embeddings, face_labels, threshold=0.3):\n",
    "    min_distance = float(\"inf\")\n",
    "    closest_label = None\n",
    "    for i, stored_embedding in enumerate(face_embeddings):\n",
    "        distance = cosine(query_embedding, stored_embedding)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_label = face_labels[i]\n",
    "    \n",
    "    if min_distance < threshold:\n",
    "        return closest_label\n",
    "    return None\n",
    "\n",
    "# Function to record audio\n",
    "def record_test():\n",
    "    #print(\"Recording for 3 seconds...\")\n",
    "    update_status(\"Recording...\")\n",
    "    window.update()  \n",
    "    audio = sd.rec(int(3 * 16000), samplerate=16000, channels=1)\n",
    "    sd.wait()\n",
    "    print(\"Recording finished.\")\n",
    "    return audio.flatten()\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(audio, sample_rate=16000):\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, 100 - mfcc.shape[1])), mode='constant')\n",
    "    return mfcc.flatten()\n",
    "\n",
    "# Function to load voice embeddings from a batch\n",
    "def load_batch(batch_num):\n",
    "    embeddings = np.load(f\"embeddings_batch_{batch_num}.npy\")\n",
    "    file_paths = np.load(f\"file_paths_batch_{batch_num}.npy\")\n",
    "    return embeddings, file_paths\n",
    "\n",
    "# Function to find the closest matching voice in a batch\n",
    "def find_closest_match_in_batch(embedding, embeddings, file_paths):\n",
    "    min_distance = float(\"inf\")\n",
    "    closest_file = None\n",
    "    for i, stored_embedding in enumerate(embeddings):\n",
    "        distance = cosine(embedding, stored_embedding)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_file = file_paths[i]\n",
    "    return closest_file, min_distance\n",
    "\n",
    "# Function to find the closest matching voice across all batches\n",
    "def find_closest_match_across_batches(real_time_embedding, num_batches, threshold=0.5):\n",
    "    closest_file = None\n",
    "    min_distance = float(\"inf\")\n",
    "    for batch_num in range(num_batches):\n",
    "        try:\n",
    "            embeddings, file_paths = load_batch(batch_num)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Batch file embeddings_batch_{batch_num}.npy not found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        batch_closest_file, batch_min_distance = find_closest_match_in_batch(real_time_embedding, embeddings, file_paths)\n",
    "        if batch_min_distance < min_distance:\n",
    "            min_distance = batch_min_distance\n",
    "            closest_file = batch_closest_file\n",
    "    \n",
    "    if min_distance < threshold:\n",
    "        return closest_file\n",
    "    return None\n",
    "\n",
    "# Function to trigger voice recording and verification\n",
    "def start_voice_verification():\n",
    "    record_button.pack_forget()  # Hide the button while recording\n",
    "    threading.Thread(target=voice_verification_thread).start()\n",
    "\n",
    "def voice_verification_thread():\n",
    "    real_time_audio = record_test()\n",
    "    real_time_embedding = extract_mfcc(real_time_audio)\n",
    "    closest_file = find_closest_match_across_batches(real_time_embedding, num_batches=43, threshold=0.5)\n",
    "\n",
    "    if closest_file:\n",
    "        update_status(f\"Access Granted: Voice recognized from {closest_file}\")\n",
    "        refresh_display_after_unlock()\n",
    "    else:\n",
    "        update_status(\"Access Denied: No Voice Match Found\")\n",
    "\n",
    "\n",
    "def real_time_detection():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        update_status(\"Error: Could not open video capture.\")\n",
    "        return\n",
    "\n",
    "    processing = True\n",
    "    while processing:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame is None:\n",
    "            update_status(\"Error: Failed to capture image.\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            liveness_score = predict_liveness(frame, liveness_model)\n",
    "            liveness_threshold = 0.5\n",
    "            if liveness_score > liveness_threshold:\n",
    "                faces = detect_faces(frame)\n",
    "                if len(faces) > 0:\n",
    "                    face_images = extract_faces(frame, faces)\n",
    "                    for face in face_images:\n",
    "                        face_embedding = get_embedding(face, face_model)\n",
    "                        matching_label = find_closest_face(face_embedding, face_embeddings, face_labels, threshold=0.3)\n",
    "                        \n",
    "                        if matching_label:\n",
    "                            update_status(f\"Access Granted: {matching_label} recognized\")\n",
    "                            processing = False\n",
    "                            refresh_display_after_unlock()\n",
    "                        else:\n",
    "                            update_status(\"Face Not Recognized: Proceeding to Voice Verification\")\n",
    "                            record_button.pack(pady=20)  # Show the button when needed\n",
    "                            processing = False  # Stop processing to wait for voice verification\n",
    "                else:\n",
    "                    update_status(\"No face detected.\")\n",
    "                    processing = False\n",
    "            else:\n",
    "                update_status(\"Access Denied: Fake Image Detected\")\n",
    "                processing = False\n",
    "            \n",
    "            # Convert frame for display\n",
    "            cv2_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(cv2_frame)\n",
    "            img = ImageTk.PhotoImage(img)\n",
    "            video_label.config(image=img)\n",
    "            video_label.image = img\n",
    "\n",
    "        except cv2.error as e:\n",
    "            update_status(f\"Error processing frame: {e}\")\n",
    "            break\n",
    "        \n",
    "        window.update()  # Ensure GUI updates while processing\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "# Function to trigger voice recording and verification\n",
    "def start_voice_verification():\n",
    "    update_status(\"Recording voice...\")\n",
    "    window.update()\n",
    "    real_time_audio = record_test()\n",
    "    real_time_embedding = extract_mfcc(real_time_audio)\n",
    "    closest_file = find_closest_match_across_batches(real_time_embedding, num_batches=43, threshold=0.5)\n",
    "\n",
    "    if closest_file:\n",
    "        update_status(f\"Access Granted: Voice recognized from {closest_file}\")\n",
    "    else:\n",
    "        update_status(\"Access Denied: No Voice Match Found\")\n",
    "# GUI-related functions\n",
    "def update_status(status_text):\n",
    "    window.after(0, lambda: status_label.config(text=status_text))\n",
    "\n",
    "def refresh_display_after_unlock():\n",
    "    # Wait for 5 seconds and then clear the displayed image\n",
    "    window.after(10000, clear_display)\n",
    "\n",
    "def clear_display():\n",
    "    video_label.config(image='')  # Clear the image\n",
    "    update_status(\"\")  # Clear the status\n",
    "\n",
    "def start_detection_thread():\n",
    "    global detection_thread\n",
    "    detection_thread = threading.Thread(target=real_time_detection, daemon=True)\n",
    "    detection_thread.start()\n",
    "\n",
    "def exit_program():\n",
    "    global running\n",
    "    running = False\n",
    "    if detection_thread.is_alive():\n",
    "        detection_thread.join()\n",
    "    window.destroy()\n",
    "\n",
    "def create_gui():\n",
    "    global window, status_label, video_label,record_button\n",
    "\n",
    "    window = tk.Tk()\n",
    "    window.title(\"Office Entry System\")\n",
    "    window.configure(bg='lightblue')\n",
    "\n",
    "    canvas = tk.Canvas(window, width=800, height=600, bg='lightblue')\n",
    "    canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "    scrollbar = tk.Scrollbar(window, orient=\"vertical\", command=canvas.yview)\n",
    "    scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "    canvas.configure(yscrollcommand=scrollbar.set)\n",
    "\n",
    "    scrollable_frame = tk.Frame(canvas, bg='lightblue')\n",
    "    canvas.create_window((0, 0), window=scrollable_frame, anchor=\"nw\")\n",
    "\n",
    "    scrollable_frame.bind(\n",
    "        \"<Configure>\",\n",
    "        lambda e: canvas.configure(scrollregion=canvas.bbox(\"all\"))\n",
    "    )\n",
    "\n",
    "    custom_font = tkfont.Font(family=\"Helvetica\", size=16, weight=\"bold\")\n",
    "\n",
    "    label = tk.Label(scrollable_frame, text=\"Office Entry System\", font=custom_font, bg=\"lightblue\", fg=\"darkblue\")\n",
    "    label.pack(pady=20)\n",
    "\n",
    "    video_label = tk.Label(scrollable_frame)\n",
    "    video_label.pack(pady=10)\n",
    "\n",
    "    start_button = tk.Button(scrollable_frame, text=\"Start Security System\", font=custom_font, command=start_detection_thread)\n",
    "    start_button.pack(pady=20)\n",
    "\n",
    "    record_button = tk.Button(scrollable_frame, text=\"Record Voice\", font=custom_font, command=start_voice_verification)\n",
    "    record_button.pack_forget() \n",
    "\n",
    "    exit_button = tk.Button(scrollable_frame, text=\"Exit\", font=custom_font, command=exit_program)\n",
    "    exit_button.pack(pady=10)\n",
    "\n",
    "    status_label = tk.Label(scrollable_frame, text=\"\", font=custom_font, bg='lightblue', fg=\"darkred\")\n",
    "    status_label.pack(pady=10)\n",
    "\n",
    "    window.protocol(\"WM_DELETE_WINDOW\", exit_program)\n",
    "\n",
    "    window.mainloop()\n",
    "\n",
    "# Start the GUI application\n",
    "create_gui()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be26eb63-e3b0-4e1d-9526-ce4c9e96bfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Liveness score: 0.9999964833259583\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "Liveness score: 0.9999985098838806\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "Liveness score: 0.9999992847442627\n",
      "1/1 [==============================] - 0s 242ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "Liveness score: 0.9999933242797852\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "Liveness score: 0.9996287226676941\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Liveness score: 0.9999896883964539\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "Liveness score: 0.9999599456787109\n",
      "1/1 [==============================] - 0s 204ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.spatial.distance import cosine\n",
    "from keras_vggface.utils import preprocess_input\n",
    "\n",
    "# Load models and embeddings\n",
    "liveness_model = load_model('liveness_model.keras', compile=False)\n",
    "face_model = load_model('face_verification_model.keras', compile=False)\n",
    "face_embeddings = np.load('my_embeddings.npy')  # Precomputed face embeddings\n",
    "face_labels = np.load('my_labels.npy')  # Labels for embeddings\n",
    "\n",
    "# Function to preprocess the face for the model\n",
    "def preprocess_frame(face_image):\n",
    "    face_resized = cv2.resize(face_image, (224, 224))  # Resize to the input size of the liveness model\n",
    "    face_normalized = face_resized / 255.0  # Normalize the pixel values\n",
    "    face_expanded = np.expand_dims(face_normalized, axis=0)  # Add batch dimension\n",
    "    return face_expanded\n",
    "\n",
    "# Liveness detection for a single face image\n",
    "def predict_liveness(face_image, model):\n",
    "    preprocessed_face = preprocess_frame(face_image)\n",
    "    prediction = model.predict(preprocessed_face)\n",
    "    return prediction[0][0]  # Return the liveness score\n",
    "\n",
    "# Function to preprocess the face for face verification\n",
    "def preprocess_image_for_verification(face_image):\n",
    "    face_resized = cv2.resize(face_image, (224, 224))  # Resize to the input size of the face model\n",
    "    face_array = np.array(face_resized, dtype=np.float32)\n",
    "    face_array = preprocess_input(face_array, version=2)\n",
    "    face_array = np.expand_dims(face_array, axis=0)  # Add batch dimension\n",
    "    return face_array\n",
    "\n",
    "# Function to extract embedding from the face\n",
    "def get_embedding(face_image, model):\n",
    "    preprocessed_face = preprocess_image_for_verification(face_image)\n",
    "    embedding = model.predict(preprocessed_face)\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Function to find the closest matching face based on embeddings\n",
    "def find_closest_face(query_embedding, face_embeddings, face_labels, threshold=0.3):\n",
    "    min_distance = float(\"inf\")\n",
    "    closest_label = None\n",
    "    for i, stored_embedding in enumerate(face_embeddings):\n",
    "        distance = cosine(query_embedding, stored_embedding)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_label = face_labels[i]\n",
    "    \n",
    "    if min_distance < threshold:\n",
    "        return closest_label\n",
    "    return None\n",
    "\n",
    "# Real-time detection and verification process\n",
    "def real_time_detection(liveness_model, face_model, face_embeddings, face_labels):\n",
    "    cap = cv2.VideoCapture(0)  # Open default camera\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video capture.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image.\")\n",
    "            break\n",
    "\n",
    "        # Step 1: Detect faces in the frame\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            for (x, y, w, h) in faces:\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "\n",
    "                # Step 2: Ignore small faces\n",
    "                if w < 100 or h < 100:\n",
    "                    print(f\"Ignoring small face of size: {w}x{h}\")\n",
    "                    continue\n",
    "\n",
    "                # Step 3: Liveness Detection\n",
    "                liveness_score = predict_liveness(face, liveness_model)\n",
    "                print(f\"Liveness score: {liveness_score}\")\n",
    "                if liveness_score > 0.5:\n",
    "                    label = 'Real'\n",
    "                    color = (0, 255, 0)  # Green for real\n",
    "                else:\n",
    "                    label = 'Fake'\n",
    "                    color = (0, 0, 255)  # Red for fake\n",
    "                    cv2.putText(frame, 'Fake Face Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                    continue  # Skip to the next face if fake\n",
    "\n",
    "                # Step 4: Face Verification (only for real faces)\n",
    "                face_embedding = get_embedding(face, face_model)\n",
    "                matching_label = find_closest_face(face_embedding, face_embeddings, face_labels, threshold=0.3)\n",
    "                if matching_label:\n",
    "                    cv2.putText(frame, f'Access Granted: {matching_label}', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "                else:\n",
    "                    cv2.putText(frame, 'Access Denied', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "                # Draw bounding box around face\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "\n",
    "        else:\n",
    "            print(\"No face detected.\")\n",
    "        \n",
    "        # Step 5: Show the frame with bounding boxes and labels\n",
    "        cv2.imshow('Liveness Detection and Face Verification', frame)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run real-time detection with liveness and face verification\n",
    "real_time_detection(liveness_model, face_model, face_embeddings, face_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962767c-11e3-4a33-9f95-47b85e770a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 205ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 325ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import threading\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "import tkinter as tk\n",
    "from tkinter import font as tkfont\n",
    "from PIL import Image, ImageTk\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.spatial.distance import cosine\n",
    "from keras_vggface.utils import preprocess_input\n",
    "\n",
    "# Load models and embeddings\n",
    "liveness_model = load_model('liveness_model.keras', compile=False)\n",
    "face_model = load_model('face_verification_model.keras', compile=False)\n",
    "face_embeddings = np.load('my_embeddings.npy')  # Precomputed face embeddings\n",
    "face_labels = np.load('my_labels.npy')  # Labels for embeddings\n",
    "\n",
    "# Global variables for GUI\n",
    "window = None\n",
    "video_label = None\n",
    "status_label = None\n",
    "record_button = None\n",
    "\n",
    "# Preprocess face image for liveness detection\n",
    "def preprocess_frame(face_image):\n",
    "    face_resized = cv2.resize(face_image, (224, 224))\n",
    "    face_normalized = face_resized / 255.0\n",
    "    face_expanded = np.expand_dims(face_normalized, axis=0)\n",
    "    return face_expanded\n",
    "\n",
    "# Liveness detection function\n",
    "def predict_liveness(face_image, model):\n",
    "    preprocessed_face = preprocess_frame(face_image)\n",
    "    prediction = model.predict(preprocessed_face)\n",
    "    return prediction[0][0]\n",
    "\n",
    "# Preprocess face image for verification\n",
    "def preprocess_image_for_verification(face_image):\n",
    "    face_resized = cv2.resize(face_image, (224, 224))\n",
    "    face_array = np.array(face_resized, dtype=np.float32)\n",
    "    face_array = preprocess_input(face_array, version=2)\n",
    "    face_array = np.expand_dims(face_array, axis=0)\n",
    "    return face_array\n",
    "\n",
    "# Get face embedding\n",
    "def get_embedding(face_image, model):\n",
    "    preprocessed_face = preprocess_image_for_verification(face_image)\n",
    "    embedding = model.predict(preprocessed_face)\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Find closest face match\n",
    "def find_closest_face(query_embedding, face_embeddings, face_labels, threshold=0.3):\n",
    "    min_distance = float(\"inf\")\n",
    "    closest_label = None\n",
    "    for i, stored_embedding in enumerate(face_embeddings):\n",
    "        distance = cosine(query_embedding, stored_embedding)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_label = face_labels[i]\n",
    "    \n",
    "    if min_distance < threshold:\n",
    "        return closest_label\n",
    "    return None\n",
    "\n",
    "# Voice recording and verification\n",
    "def record_test():\n",
    "    update_status(\"Recording for 3 seconds...\")\n",
    "    window.update()\n",
    "    audio = sd.rec(int(3 * 16000), samplerate=16000, channels=1)\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    return audio.flatten()\n",
    "\n",
    "# Extract MFCC features from audio\n",
    "def extract_mfcc(audio, sample_rate=16000):\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, 100 - mfcc.shape[1])), mode='constant')\n",
    "    return mfcc.flatten()\n",
    "\n",
    "# Load voice embeddings\n",
    "def load_batch(batch_num):\n",
    "    embeddings = np.load(f\"embeddings_batch_{batch_num}.npy\")\n",
    "    file_paths = np.load(f\"file_paths_batch_{batch_num}.npy\")\n",
    "    return embeddings, file_paths\n",
    "\n",
    "# Find the closest voice match\n",
    "def find_closest_match_in_batch(embedding, embeddings, file_paths):\n",
    "    min_distance = float(\"inf\")\n",
    "    closest_file = None\n",
    "    for i, stored_embedding in enumerate(embeddings):\n",
    "        distance = cosine(embedding, stored_embedding)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_file = file_paths[i]\n",
    "    return closest_file, min_distance\n",
    "\n",
    "# Function to trigger voice verification\n",
    "def start_voice_verification():\n",
    "    update_status(\"Recording voice...\")\n",
    "    window.update()\n",
    "    real_time_audio = record_test()\n",
    "    real_time_embedding = extract_mfcc(real_time_audio)\n",
    "    closest_file = find_closest_match_across_batches(real_time_embedding, num_batches=43, threshold=0.5)\n",
    "\n",
    "    if closest_file:\n",
    "        update_status(f\"Access Granted: Voice recognized from {closest_file}\")\n",
    "    else:\n",
    "        update_status(\"Access Denied: No Voice Match Found\")\n",
    "\n",
    "# Voice verification across batches\n",
    "def find_closest_match_across_batches(real_time_embedding, num_batches, threshold=0.5):\n",
    "    closest_file = None\n",
    "    min_distance = float(\"inf\")\n",
    "    for batch_num in range(num_batches):\n",
    "        embeddings, file_paths = load_batch(batch_num)\n",
    "        batch_closest_file, batch_min_distance = find_closest_match_in_batch(real_time_embedding, embeddings, file_paths)\n",
    "        if batch_min_distance < min_distance:\n",
    "            min_distance = batch_min_distance\n",
    "            closest_file = batch_closest_file\n",
    "    \n",
    "    if min_distance < threshold:\n",
    "        return closest_file\n",
    "    return None\n",
    "\n",
    "# Real-time face detection, liveness, and verification\n",
    "def real_time_detection():\n",
    "    cap = cv2.VideoCapture(0)  # Open the default camera\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        update_status(\"Error: Could not open video capture.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            update_status(\"Error: Failed to capture image.\")\n",
    "            break\n",
    "\n",
    "        # Step 1: Detect faces\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            for (x, y, w, h) in faces:\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "\n",
    "                # Step 2: Ignore small faces\n",
    "                if w < 100 or h < 100:\n",
    "                    update_status(f\"Ignoring small face of size: {w}x{h}\")\n",
    "                    continue\n",
    "\n",
    "                # Step 3: Liveness detection\n",
    "                liveness_score = predict_liveness(face, liveness_model)\n",
    "                if liveness_score > 0.5:\n",
    "                    label = 'Real'\n",
    "                    color = (0, 255, 0)  # Green for real\n",
    "                else:\n",
    "                    label = 'Fake'\n",
    "                    color = (0, 0, 255)  # Red for fake\n",
    "                    update_status('Fake Face Detected')\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                    continue\n",
    "\n",
    "                # Step 4: Face verification\n",
    "                face_embedding = get_embedding(face, face_model)\n",
    "                matching_label = find_closest_face(face_embedding, face_embeddings, face_labels, threshold=0.3)\n",
    "                if matching_label:\n",
    "                    update_status(f'Access Granted: {matching_label}')\n",
    "                else:\n",
    "                    update_status('Face Not Recognized: Proceeding to Voice Verification')\n",
    "                    record_button.pack(pady=20)\n",
    "                    return\n",
    "\n",
    "                # Draw bounding box around face\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "\n",
    "        else:\n",
    "            update_status(\"No face detected.\")\n",
    "        \n",
    "        show_frame_in_gui(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# GUI update functions\n",
    "def update_status(status_text):\n",
    "    window.after(0, lambda: status_label.config(text=status_text))\n",
    "\n",
    "def show_frame_in_gui(frame):\n",
    "    cv2_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(cv2_frame)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    video_label.imgtk = imgtk\n",
    "    video_label.configure(image=imgtk)\n",
    "\n",
    "# Start real-time detection in a separate thread\n",
    "def start_detection_thread():\n",
    "    detection_thread = threading.Thread(target=real_time_detection, daemon=True)\n",
    "    detection_thread.start()\n",
    "\n",
    "# GUI creation\n",
    "def create_gui():\n",
    "    global window, video_label, status_label, record_button\n",
    "\n",
    "    window = tk.Tk()\n",
    "    window.title(\"Office Entry System\")\n",
    "    window.configure(bg='lightblue')\n",
    "\n",
    "    canvas = tk.Canvas(window, width=800, height=600, bg='lightblue')\n",
    "    canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "    scrollbar = tk.Scrollbar(window, orient=\"vertical\", command=canvas.yview)\n",
    "    scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "    canvas.configure(yscrollcommand=scrollbar.set)\n",
    "\n",
    "    scrollable_frame = tk.Frame(canvas, bg='lightblue')\n",
    "    canvas.create_window((0, 0), window=scrollable_frame, anchor=\"nw\")\n",
    "\n",
    "    scrollable_frame.bind(\n",
    "        \"<Configure>\",\n",
    "        lambda e: canvas.configure(scrollregion=canvas.bbox(\"all\"))\n",
    "    )\n",
    "\n",
    "    custom_font = tkfont.Font(family=\"Helvetica\", size=16, weight=\"bold\")\n",
    "\n",
    "    label = tk.Label(scrollable_frame, text=\"Office Entry System\", font=custom_font, bg=\"lightblue\", fg=\"darkblue\")\n",
    "    label.pack(pady=20)\n",
    "\n",
    "    video_label = tk.Label(scrollable_frame)\n",
    "    video_label.pack(pady=10)\n",
    "\n",
    "    start_button = tk.Button(scrollable_frame, text=\"Start Security System\", font=custom_font, command=start_detection_thread)\n",
    "    start_button.pack(pady=20)\n",
    "\n",
    "    record_button = tk.Button(scrollable_frame, text=\"Record Voice\", font=custom_font, command=start_voice_verification)\n",
    "    record_button.pack_forget()  # Initially\n",
    "    \n",
    "\n",
    "    exit_button = tk.Button(scrollable_frame, text=\"Exit\", font=custom_font, command=exit_program)\n",
    "    exit_button.pack(pady=10)\n",
    "\n",
    "    status_label = tk.Label(scrollable_frame, text=\"\", font=custom_font, bg='lightblue', fg=\"darkred\")\n",
    "    status_label.pack(pady=10)\n",
    "\n",
    "    window.protocol(\"WM_DELETE_WINDOW\", exit_program)\n",
    "\n",
    "    window.mainloop()\n",
    "\n",
    "# Start the GUI application\n",
    "create_gui()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f4f0e-595b-4b59-b66c-7ad56da6db7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
